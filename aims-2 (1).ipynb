{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2186453,"sourceType":"datasetVersion","datasetId":1312522}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 1: Setup and Installations\n# This first cell sets up the environment by installing necessary libraries and cloning the required repositories.\n# We will use PyTorch and torchvision, which are a great combination for object detection.\n# MattNet is a research model, so we'll clone a public implementation from GitHub.\n\n!pip install torch torchvision\n!git clone https://github.com/lichengunc/MAttNet.git\n# Note: You may need to compile some custom C++/CUDA extensions depending on the MattNet implementation.\n# This can be done by navigating into the lib directory of the cloned repository and running:\n# cd MAttNet/lib\n# python setup.py build develop\n\n# Import all necessary libraries for the rest of the notebook.\nimport os\nimport json\nimport torch\nimport torchvision\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torch.utils.data import Dataset, DataLoader\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n# Define the device to use (GPU if available).\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(f\"Using device: {device}\")\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-15T14:35:36.659697Z","iopub.execute_input":"2025-08-15T14:35:36.659890Z","iopub.status.idle":"2025-08-15T14:37:11.597446Z","shell.execute_reply.started":"2025-08-15T14:35:36.659873Z","shell.execute_reply":"2025-08-15T14:37:11.596701Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\nSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\nCloning into 'MAttNet'...\nremote: Enumerating objects: 337, done.\u001b[K\nremote: Counting objects: 100% (12/12), done.\u001b[K\nremote: Compressing objects: 100% (12/12), done.\u001b[K\nremote: Total 337 (delta 6), reused 1 (delta 0), pack-reused 325 (from 1)\u001b[K\nReceiving objects: 100% (337/337), 2.24 MiB | 14.07 MiB/s, done.\nResolving deltas: 100% (184/184), done.\nUsing device: cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\n# Cell 2: Data Linking and Loading\n# This section has been updated to specifically handle the Kaggle file path structure you provided.\n# We will no longer use kagglehub but instead assume the dataset is mounted.\n\n# --- START OF FILE PATH FIX ---\n# Import necessary modules here to ensure they are always available.\nimport os\nimport json\n\n# Define the root directory of the Kaggle dataset.\nKAGGLE_DATA_ROOT = \"/kaggle/input/visualgenome\"\nprint(f\"Using Kaggle data root: {KAGGLE_DATA_ROOT}\")\n\n# We will now search for the specific file and image directories,\n# as they are located in different subdirectories.\nregion_graphs_path = None\nimage_data_path = None\nIMAGE_DIR_1 = None\nIMAGE_DIR_2 = None\n\nprint(\"Searching for data files and image directories within the downloaded dataset...\")\n\n# Walk the directory tree to find all necessary files and directories.\nfor root, dirs, files in os.walk(KAGGLE_DATA_ROOT):\n    # Find the path for region_graphs.json\n    if 'region_graphs.json' in files and region_graphs_path is None:\n        region_graphs_path = os.path.join(root, 'region_graphs.json')\n    \n    # Find the path for image_data.json\n    if 'image_data.json' in files and image_data_path is None:\n        image_data_path = os.path.join(root, 'image_data.json')\n\n    # Find the image directories.\n    if 'VG_100K' in dirs and IMAGE_DIR_1 is None:\n        IMAGE_DIR_1 = os.path.join(root, 'VG_100K')\n    if 'VG_100K_2' in dirs and IMAGE_DIR_2 is None:\n        IMAGE_DIR_2 = os.path.join(root, 'VG_100K_2')\n\n    # If all paths are found, we can stop the walk early to save time.\n    if region_graphs_path and image_data_path and IMAGE_DIR_1 and IMAGE_DIR_2:\n        break\n\nif not region_graphs_path:\n    raise FileNotFoundError(\"Could not locate the file 'region_graphs.json'.\")\nif not image_data_path:\n    raise FileNotFoundError(\"Could not locate the file 'image_data.json'.\")\nif not IMAGE_DIR_1:\n    raise FileNotFoundError(\"Could not locate the directory 'VG_100K'.\")\nif not IMAGE_DIR_2:\n    raise FileNotFoundError(\"Could not locate the directory 'VG_100K_2'.\")\n\nprint(f\"region_graphs.json found at: {region_graphs_path}\")\nprint(f\"image_data.json found at: {image_data_path}\")\nprint(f\"Image directory 1 found in: {IMAGE_DIR_1}\")\nprint(f\"Image directory 2 found in: {IMAGE_DIR_2}\")\n\n# Load the region graphs JSON data, which contains the annotations.\nwith open(region_graphs_path, 'r') as f:\n    region_graphs = json.load(f)\n\n# Load the image data JSON data (if needed for metadata, e.g., image dimensions).\nwith open(image_data_path, 'r') as f:\n    image_data = json.load(f)\n\n# --- END OF FILE PATH FIX ---\n\nprint(f\"Loaded {len(region_graphs)} region graph entries.\")\nprint(f\"Loaded {len(image_data)} image data entries.\")\n\n# Create a mapping from image ID to its file path for quick lookup.\n# We need to check both image directories.\nimage_id_to_path = {}\nfor img in image_data:\n    image_id = img['image_id']\n    path1 = os.path.join(IMAGE_DIR_1, f\"{image_id}.jpg\")\n    path2 = os.path.join(IMAGE_DIR_2, f\"{image_id}.jpg\")\n    if os.path.exists(path1):\n        image_id_to_path[image_id] = path1\n    elif os.path.exists(path2):\n        image_id_to_path[image_id] = path2\n\n# Prepare a list of all images we want to process.\n# Due to the large size, it's highly recommended to start with a subset.\n# For example, let's take the first 1000 images. For the final training, you would use all 50,000.\n# The code is designed to scale; just change the slice.\nsubset_size = 50000\nimage_ids = list(image_id_to_path.keys())[:subset_size]\n\nif not image_ids:\n    raise ValueError(\"No images were found in the dataset. Please check the dataset structure.\")\nprint(f\"Using a subset of {len(image_ids)} images for training.\")\n\n\n# Create a dictionary to map image IDs to their descriptions for easier access.\nimage_annotations = {}\nfor entry in region_graphs:\n    image_id = entry['image_id']\n    if image_id in image_id_to_path:\n        # Each entry has a list of regions. We'll store them.\n        image_annotations[image_id] = entry['regions']\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T15:13:15.795419Z","iopub.execute_input":"2025-08-15T15:13:15.795706Z","iopub.status.idle":"2025-08-15T15:19:31.751707Z","shell.execute_reply.started":"2025-08-15T15:13:15.795684Z","shell.execute_reply":"2025-08-15T15:19:31.751067Z"}},"outputs":[{"name":"stdout","text":"Using Kaggle data root: /kaggle/input/visualgenome\nSearching for data files and image directories within the downloaded dataset...\nregion_graphs.json found at: /kaggle/input/visualgenome/region_graphs.json/region_graphs.json\nimage_data.json found at: /kaggle/input/visualgenome/image_data.json/image_data.json\nImage directory 1 found in: /kaggle/input/visualgenome/images/VG_100K\nImage directory 2 found in: /kaggle/input/visualgenome/images2/VG_100K_2\nLoaded 108077 region graph entries.\nLoaded 108077 image data entries.\nUsing a subset of 50000 images for training.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"\n# Cell 3: Custom Dataset Class\n# This class handles loading each image and its corresponding annotations on-the-fly.\n# This is a critical step for memory efficiency with large datasets.\n\nclass VisualGenomeDataset(Dataset):\n    def __init__(self, image_ids, image_annotations, image_id_to_path, transforms=None):\n        self.image_ids = image_ids\n        self.image_annotations = image_annotations\n        self.image_id_to_path = image_id_to_path\n        self.transforms = transforms\n\n    def __getitem__(self, idx):\n        # Use a try-except block to gracefully handle potential issues with image loading or annotations\n        # which can sometimes be corrupt in large datasets.\n        try:\n            # Get the image ID for the current index.\n            image_id = self.image_ids[idx]\n            \n            # Open the image file.\n            img_path = self.image_id_to_path[image_id]\n            img = Image.open(img_path).convert(\"RGB\")\n            \n            # Get the annotations (regions) for this image.\n            regions = self.image_annotations.get(image_id, [])\n\n            # Parse the bounding boxes and labels from the regions.\n            boxes = []\n            labels = []\n            # The first class (0) is traditionally reserved for the background in PyTorch.\n            # We will use class 1 for all objects.\n            for region in regions:\n                x, y, w, h = region['x'], region['y'], region['width'], region['height']\n                # Convert (x, y, w, h) to (x_min, y_min, x_max, y_max)\n                boxes.append([x, y, x + w, y + h])\n                # For simplicity, we are assigning all as a single class (e.g., 'object').\n                labels.append(1) \n            \n            # If no objects are found, use a dummy box and label to prevent errors.\n            if not boxes:\n                boxes = torch.zeros((0, 4), dtype=torch.float32)\n                labels = torch.zeros(0, dtype=torch.int64)\n            else:\n                boxes = torch.as_tensor(boxes, dtype=torch.float32)\n                labels = torch.as_tensor(labels, dtype=torch.int64)\n\n            target = {}\n            target[\"boxes\"] = boxes\n            target[\"labels\"] = labels\n            target[\"image_id\"] = torch.tensor([image_id])\n\n            # Apply transformations if they are defined.\n            if self.transforms is not None:\n                # FIX: This is the line that was causing the error.\n                # The transform `ToTensor` is designed to be applied to a single image.\n                # Passing `img, target` caused the `TypeError`. We must only pass the image.\n                img = self.transforms(img)\n\n            return img, target\n        \n        except Exception as e:\n            print(f\"Error processing image ID {self.image_ids[idx]}: {e}\")\n            # Return a valid empty sample to prevent the DataLoader from crashing.\n            return (torch.zeros((3, 224, 224), dtype=torch.float32), \n                    {'boxes': torch.zeros((0, 4), dtype=torch.float32), 'labels': torch.zeros(0, dtype=torch.int64)})\n\n\n    def __len__(self):\n        return len(self.image_ids)\n\n# Define transformations for the dataset.\n# The `ToTensor` transform should only operate on the image.\n# We will return the transform directly here.\ndef get_transform():\n    return torchvision.transforms.ToTensor()\n\n# Create the dataset and a data loader. The data loader will handle batching and shuffling.\nif 'image_ids' in locals() and 'image_annotations' in locals() and 'image_id_to_path' in locals():\n    dataset = VisualGenomeDataset(image_ids, image_annotations, image_id_to_path, transforms=get_transform())\n\n    # A custom collate function is needed because the images have different numbers of objects.\n    def collate_fn(batch):\n        # Filter out any None values returned by the dataset's __getitem__ method in case of errors.\n        batch = [item for item in batch if item is not None]\n        if not batch:\n            return None, None\n        return tuple(zip(*batch))\n\n    data_loader = DataLoader(\n        dataset,\n        batch_size=2,  # Set a small batch size due to the large image size and memory constraints.\n        shuffle=True,\n        # Setting num_workers to 0 to prevent memory crashes.\n        # This will make data loading slower, but more stable.\n        # You can increase this value if you are sure your environment has enough memory.\n        num_workers=0,\n        collate_fn=collate_fn\n    )\nelse:\n    print(\"Skipping Dataset and DataLoader creation due to missing data.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T15:38:21.722935Z","iopub.execute_input":"2025-08-15T15:38:21.723514Z","iopub.status.idle":"2025-08-15T15:38:21.734214Z","shell.execute_reply.started":"2025-08-15T15:38:21.723494Z","shell.execute_reply":"2025-08-15T15:38:21.733533Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"\n# Cell 4: Model Initialization (Faster R-CNN with MattNet Concept)\n# We will use a pre-trained Faster R-CNN model as our base.\n# MattNet is a visual-textual model that is complex to implement from scratch.\n# The core idea is to integrate a language-based reasoning component.\n# This is a simplified approach, as the full MattNet requires a separate language model.\n# A full implementation would involve replacing the `FastRCNNPredictor` with a custom module from the MattNet repo.\n\n# The number of classes: 1 for 'object' and 1 for the background.\nnum_classes = 2\n\n# Load a pre-trained Faster R-CNN model from torchvision.\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n\n# Replace the box predictor to match our number of classes.\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n# If you were to fully implement MattNet, you would replace this with a more complex head\n# that also takes the language description as input. This requires a deeper dive into the MattNet repo's code.\n# For example, you might create a new class that inherits from FastRCNNPredictor and adds a text encoder.\n# from MAttNet_repo.model import CustomPredictor\n# model.roi_heads.box_predictor = CustomPredictor(...)\n\nmodel.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T15:38:32.860800Z","iopub.execute_input":"2025-08-15T15:38:32.861063Z","iopub.status.idle":"2025-08-15T15:38:33.532535Z","shell.execute_reply.started":"2025-08-15T15:38:32.861045Z","shell.execute_reply":"2025-08-15T15:38:33.531812Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"FasterRCNN(\n  (transform): GeneralizedRCNNTransform(\n      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n  )\n  (backbone): BackboneWithFPN(\n    (body): IntermediateLayerGetter(\n      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n      (relu): ReLU(inplace=True)\n      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n      (layer1): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): FrozenBatchNorm2d(256, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer2): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(512, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer3): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(1024, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (4): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (5): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer4): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(2048, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n    )\n    (fpn): FeaturePyramidNetwork(\n      (inner_blocks): ModuleList(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (2): Conv2dNormActivation(\n          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (3): Conv2dNormActivation(\n          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (layer_blocks): ModuleList(\n        (0-3): 4 x Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n      )\n      (extra_blocks): LastLevelMaxPool()\n    )\n  )\n  (rpn): RegionProposalNetwork(\n    (anchor_generator): AnchorGenerator()\n    (head): RPNHead(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): ReLU(inplace=True)\n        )\n      )\n      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n    )\n  )\n  (roi_heads): RoIHeads(\n    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n    (box_head): TwoMLPHead(\n      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n    )\n    (box_predictor): FastRCNNPredictor(\n      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n    )\n  )\n)"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"\n# Cell 5: Training Pipeline\n# This cell contains the main training loop. We will train for a few epochs.\n# You can adjust the number of epochs and the learning rate.\n\n# Construct an optimizer.\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n\n# Optional: Learning rate scheduler.\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n\n# Number of epochs. Start with a small number and increase it as you monitor progress.\nnum_epochs = 5\n\nfor epoch in range(num_epochs):\n    # Set the model to training mode.\n    model.train()\n    \n    # Use tqdm to show a progress bar.\n    progress_bar = tqdm(data_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n    \n    for images, targets in progress_bar:\n        # Move images and targets to the device.\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        # Get the losses from the model.\n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n        \n        # Backpropagation and optimization.\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n        \n        # Update the progress bar with the current loss.\n        progress_bar.set_postfix(loss=losses.item())\n        \n    # Update the learning rate scheduler (if used).\n    if lr_scheduler is not None:\n        lr_scheduler.step()\n        \n    print(f\"Epoch {epoch+1} finished. Total Loss: {losses.item()}\")\n\n# Save the trained model.\ntorch.save(model.state_dict(), 'faster_rcnn_visual_genome.pth')\nprint(\"Training complete. Model saved.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T15:38:42.046744Z","iopub.execute_input":"2025-08-15T15:38:42.047021Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/5:  49%|████▉     | 12293/25000 [2:21:05<2:22:57,  1.48it/s, loss=1.78]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}