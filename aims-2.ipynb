{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2186453,"sourceType":"datasetVersion","datasetId":1312522}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 1: Setup and Installations\n# This first cell sets up the environment by installing necessary libraries and cloning the required repositories.\n# We will use PyTorch and torchvision, which are a great combination for object detection.\n# MattNet is a research model, so we'll clone a public implementation from GitHub.\n\n!pip install torch torchvision\n!git clone https://github.com/lichengunc/MAttNet.git\n# Note: You may need to compile some custom C++/CUDA extensions depending on the MattNet implementation.\n# This can be done by navigating into the lib directory of the cloned repository and running:\n# cd MAttNet/lib\n# python setup.py build develop\n\n# Import all necessary libraries for the rest of the notebook.\nimport os\nimport json\nimport torch\nimport torchvision\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torch.utils.data import Dataset, DataLoader\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n# Define the device to use (GPU if available).\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(f\"Using device: {device}\")\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-17T18:38:46.156407Z","iopub.execute_input":"2025-08-17T18:38:46.156657Z","iopub.status.idle":"2025-08-17T18:40:15.826076Z","shell.execute_reply.started":"2025-08-17T18:38:46.156631Z","shell.execute_reply":"2025-08-17T18:40:15.825288Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m99.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\nSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\nCloning into 'MAttNet'...\nremote: Enumerating objects: 337, done.\u001b[K\nremote: Counting objects: 100% (12/12), done.\u001b[K\nremote: Compressing objects: 100% (12/12), done.\u001b[K\nremote: Total 337 (delta 6), reused 1 (delta 0), pack-reused 325 (from 1)\u001b[K\nReceiving objects: 100% (337/337), 2.24 MiB | 14.33 MiB/s, done.\nResolving deltas: 100% (184/184), done.\nUsing device: cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\n# Cell 2: Data Linking and Loading\n# This section has been updated to specifically handle the Kaggle file path structure you provided.\n# We will no longer use kagglehub but instead assume the dataset is mounted.\n\n# --- START OF FILE PATH FIX ---\n# Import necessary modules here to ensure they are always available.\nimport os\nimport json\n\n# Define the root directory of the Kaggle dataset.\nKAGGLE_DATA_ROOT = \"/kaggle/input/visualgenome\"\nprint(f\"Using Kaggle data root: {KAGGLE_DATA_ROOT}\")\n\n# We will now search for the specific file and image directories,\n# as they are located in different subdirectories.\nregion_graphs_path = None\nimage_data_path = None\nIMAGE_DIR_1 = None\nIMAGE_DIR_2 = None\n\nprint(\"Searching for data files and image directories within the downloaded dataset...\")\n\n# Walk the directory tree to find all necessary files and directories.\nfor root, dirs, files in os.walk(KAGGLE_DATA_ROOT):\n    # Find the path for region_graphs.json\n    if 'region_graphs.json' in files and region_graphs_path is None:\n        region_graphs_path = os.path.join(root, 'region_graphs.json')\n    \n    # Find the path for image_data.json\n    if 'image_data.json' in files and image_data_path is None:\n        image_data_path = os.path.join(root, 'image_data.json')\n\n    # Find the image directories.\n    if 'VG_100K' in dirs and IMAGE_DIR_1 is None:\n        IMAGE_DIR_1 = os.path.join(root, 'VG_100K')\n    if 'VG_100K_2' in dirs and IMAGE_DIR_2 is None:\n        IMAGE_DIR_2 = os.path.join(root, 'VG_100K_2')\n\n    # If all paths are found, we can stop the walk early to save time.\n    if region_graphs_path and image_data_path and IMAGE_DIR_1 and IMAGE_DIR_2:\n        break\n\nif not region_graphs_path:\n    raise FileNotFoundError(\"Could not locate the file 'region_graphs.json'.\")\nif not image_data_path:\n    raise FileNotFoundError(\"Could not locate the file 'image_data.json'.\")\nif not IMAGE_DIR_1:\n    raise FileNotFoundError(\"Could not locate the directory 'VG_100K'.\")\nif not IMAGE_DIR_2:\n    raise FileNotFoundError(\"Could not locate the directory 'VG_100K_2'.\")\n\nprint(f\"region_graphs.json found at: {region_graphs_path}\")\nprint(f\"image_data.json found at: {image_data_path}\")\nprint(f\"Image directory 1 found in: {IMAGE_DIR_1}\")\nprint(f\"Image directory 2 found in: {IMAGE_DIR_2}\")\n\n# Load the region graphs JSON data, which contains the annotations.\nwith open(region_graphs_path, 'r') as f:\n    region_graphs = json.load(f)\n\n# Load the image data JSON data (if needed for metadata, e.g., image dimensions).\nwith open(image_data_path, 'r') as f:\n    image_data = json.load(f)\n\n# --- END OF FILE PATH FIX ---\n\nprint(f\"Loaded {len(region_graphs)} region graph entries.\")\nprint(f\"Loaded {len(image_data)} image data entries.\")\n\n# Create a mapping from image ID to its file path for quick lookup.\n# We need to check both image directories.\nimage_id_to_path = {}\nfor img in image_data:\n    image_id = img['image_id']\n    path1 = os.path.join(IMAGE_DIR_1, f\"{image_id}.jpg\")\n    path2 = os.path.join(IMAGE_DIR_2, f\"{image_id}.jpg\")\n    if os.path.exists(path1):\n        image_id_to_path[image_id] = path1\n    elif os.path.exists(path2):\n        image_id_to_path[image_id] = path2\n\n# Prepare a list of all images we want to process.\n# Due to the large size, it's highly recommended to start with a subset.\n# For example, let's take the first 1000 images. For the final training, you would use all 50,000.\n# The code is designed to scale; just change the slice.\nsubset_size = 50000\nimage_ids = list(image_id_to_path.keys())[:subset_size]\n\nif not image_ids:\n    raise ValueError(\"No images were found in the dataset. Please check the dataset structure.\")\nprint(f\"Using a subset of {len(image_ids)} images for training.\")\n\n\n# Create a dictionary to map image IDs to their descriptions for easier access.\nimage_annotations = {}\nfor entry in region_graphs:\n    image_id = entry['image_id']\n    if image_id in image_id_to_path:\n        # Each entry has a list of regions. We'll store them.\n        image_annotations[image_id] = entry['regions']\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-17T19:16:27.239640Z","iopub.execute_input":"2025-08-17T19:16:27.240365Z","execution_failed":"2025-08-17T20:06:45.637Z"}},"outputs":[{"name":"stdout","text":"Using Kaggle data root: /kaggle/input/visualgenome\nSearching for data files and image directories within the downloaded dataset...\nregion_graphs.json found at: /kaggle/input/visualgenome/region_graphs.json/region_graphs.json\nimage_data.json found at: /kaggle/input/visualgenome/image_data.json/image_data.json\nImage directory 1 found in: /kaggle/input/visualgenome/images/VG_100K\nImage directory 2 found in: /kaggle/input/visualgenome/images2/VG_100K_2\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Cell 3: Custom Dataset Class\n# This class handles loading each image and its corresponding annotations on-the-fly.\n# This is a critical step for memory efficiency with large datasets.\n\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nfrom PIL import Image\nimport torch\nimport os # Ensure os is imported for num_cpus\n\nclass VisualGenomeDataset(Dataset):\n    def __init__(self, image_ids, image_annotations, image_id_to_path, transforms=None):\n        self.image_ids = image_ids\n        self.image_annotations = image_annotations\n        self.image_id_to_path = image_id_to_path\n        self.transforms = transforms\n\n    def __getitem__(self, idx):\n        # Use a try-except block to gracefully handle potential issues with image loading or annotations\n        # which can sometimes be corrupt in large datasets.\n        try:\n            # Get the image ID for the current index.\n            image_id = self.image_ids[idx]\n            \n            # Open the image file.\n            img_path = self.image_id_to_path[image_id]\n            img = Image.open(img_path).convert(\"RGB\")\n            \n            # Get the annotations (regions) for this image.\n            regions = self.image_annotations.get(image_id, [])\n\n            # Parse the bounding boxes and labels from the regions.\n            boxes = []\n            labels = []\n            # The first class (0) is traditionally reserved for the background in PyTorch.\n            # We will use class 1 for all objects.\n            for region in regions:\n                x, y, w, h = region['x'], region['y'], region['width'], region['height']\n                # Convert (x, y, w, h) to (x_min, y_min, x_max, y_max)\n                boxes.append([x, y, x + w, y + h])\n                # For simplicity, we are assigning all as a single class (e.g., 'object').\n                labels.append(1) \n            \n            # If no objects are found, use a dummy box and label to prevent errors.\n            if not boxes:\n                boxes = torch.zeros((0, 4), dtype=torch.float32)\n                labels = torch.zeros(0, dtype=torch.int64)\n            else:\n                boxes = torch.as_tensor(boxes, dtype=torch.float32)\n                labels = torch.as_tensor(labels, dtype=torch.int64)\n\n            target = {}\n            target[\"boxes\"] = boxes\n            target[\"labels\"] = labels\n            target[\"image_id\"] = torch.tensor([image_id])\n\n            # Apply transformations if they are defined.\n            if self.transforms is not None:\n                img = self.transforms(img)\n\n            return img, target\n        \n        except Exception as e:\n            print(f\"Error processing image ID {self.image_ids[idx]}: {e}\")\n            # Return a valid empty sample to prevent the DataLoader from crashing.\n            return (torch.zeros((3, 224, 224), dtype=torch.float32), \n                    {'boxes': torch.zeros((0, 4), dtype=torch.float32), 'labels': torch.zeros(0, dtype=torch.int64)})\n\n\n    def __len__(self):\n        return len(self.image_ids)\n\n# Define transformations for the dataset.\ndef get_transform():\n    return torchvision.transforms.ToTensor()\n\n# Create the dataset and a data loader. The data loader will handle batching and shuffling.\nif 'image_ids' in locals() and 'image_annotations' in locals() and 'image_id_to_path' in locals():\n    dataset = VisualGenomeDataset(image_ids, image_annotations, image_id_to_path, transforms=get_transform())\n\n    # A custom collate function is needed because the images have different numbers of objects.\n    def collate_fn(batch):\n        return tuple(zip(*batch))\n\n    # --- OPTIMIZED DATALOADER ---\n# Using multiple workers to load data in parallel and pinning memory for faster GPU transfer.\nnum_cpus = os.cpu_count()\ndata_loader = DataLoader(\n    dataset,\n    batch_size=2,  # CHANGED: Lowered from 4 to 2 to reduce memory usage.\n    shuffle=True,\n    num_workers=min(num_cpus, 8),\n    pin_memory=True,\n    collate_fn=collate_fn\n)\nprint(f\"DataLoader created with batch_size=2 and {min(num_cpus, 8)} workers.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 3.5: Compiler Configuration\n# We add this cell to optimize the behavior of torch.compile for this specific model.\n\nimport torch\n\n# Increase the cache limit for compiled models to handle variable image sizes.\ntorch._dynamo.config.cache_size_limit = 64\n\n# Instruct dynamo to capture scalar outputs like .item(), preventing graph breaks.\ntorch._dynamo.config.capture_scalar_outputs = True\n\nprint(\"PyTorch compiler configured for better performance.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 4: Model Initialization (Final Version)\n\n# The number of classes: 1 for 'object' and 1 for the background.\nnum_classes = 2\n\n# Load a pre-trained Faster R-CNN model from torchvision.\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights='DEFAULT')\n\n# Replace the box predictor to match our number of classes.\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\nmodel.to(device)\n\n# --- The torch.compile lines have been removed to prevent the BackendCompilerFailed error ---\nprint(\"Model loaded successfully. Proceeding without torch.compile.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 5: Training Pipeline (Final Optimized Version)\nimport torch\nfrom tqdm import tqdm\n\n# --- Hyperparameters ---\nnum_epochs = 5\n# Effective batch size = batch_size * accumulation_steps (2 * 16 = 32)\naccumulation_steps = 16 # CHANGED: Increased from 8 to 16 to maintain training stability.\n\n# --- Optimizer & Scheduler ---\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\ntotal_steps = len(data_loader) * num_epochs \nlr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001, total_steps=total_steps)\n\n# --- Mixed Precision Scaler (Updated API) ---\n# Use torch.amp.GradScaler for the modern, non-deprecated API.\nscaler = torch.amp.GradScaler('cuda')\n\n# --- Main Training Loop ---\nprint(\"Starting optimized training...\")\nfor epoch in range(num_epochs):\n    model.train()\n    progress_bar = tqdm(data_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n    \n    for i, batch in enumerate(progress_bar):\n        if batch[0] is None:\n            continue\n        images, targets = batch\n        \n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        # Use autocast with the modern, non-deprecated API.\n        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n            loss_dict = model(images, targets)\n            losses = sum(loss for loss in loss_dict.values())\n            \n            if accumulation_steps > 1:\n                losses = losses / accumulation_steps\n\n        scaler.scale(losses).backward()\n\n        if (i + 1) % accumulation_steps == 0:\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n        \n        # NOTE: The warning about calling lr_scheduler.step() before optimizer.step()\n        # is expected and correct behavior when using OneCycleLR with gradient accumulation.\n        # We want the learning rate to update smoothly every batch.\n        lr_scheduler.step()\n        \n        progress_bar.set_postfix(loss=(losses.item() * accumulation_steps), lr=optimizer.param_groups[0]['lr'])\n\n    print(f\"Epoch {epoch+1} finished.\")\n\n# --- Save Model ---\ntorch.save(model.state_dict(), 'faster_rcnn_visual_genome_optimized.pth')\nprint(\"Training complete. Optimized model saved.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}