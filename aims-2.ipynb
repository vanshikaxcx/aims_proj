{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ab59394",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-20T19:53:22.051846Z",
     "iopub.status.busy": "2025-08-20T19:53:22.051595Z",
     "iopub.status.idle": "2025-08-20T19:54:42.432879Z",
     "shell.execute_reply": "2025-08-20T19:54:42.431983Z"
    },
    "papermill": {
     "duration": 80.385732,
     "end_time": "2025-08-20T19:54:42.434231",
     "exception": false,
     "start_time": "2025-08-20T19:53:22.048499",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\r\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.5.1)\r\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\r\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\r\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\r\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\r\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\r\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\r\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\r\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\r\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\r\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\r\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\r\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\r\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m106.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\r\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\r\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-curand-cu12\r\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\r\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\r\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\r\n",
      "  Attempting uninstall: nvidia-cufft-cu12\r\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\r\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\r\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\r\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\r\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\r\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\r\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cublas-cu12\r\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\r\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\r\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\r\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\r\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\r\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\r\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\r\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\r\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\r\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\r\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\r\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\r\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\r\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\r\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\r\n",
      "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\r\n",
      "Cloning into 'MAttNet'...\r\n",
      "remote: Enumerating objects: 337, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (12/12), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (12/12), done.\u001b[K\r\n",
      "remote: Total 337 (delta 6), reused 1 (delta 0), pack-reused 325 (from 1)\u001b[K\r\n",
      "Receiving objects: 100% (337/337), 2.24 MiB | 29.03 MiB/s, done.\r\n",
      "Resolving deltas: 100% (184/184), done.\r\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup and Installations\n",
    "# This first cell sets up the environment by installing necessary libraries and cloning the required repositories.\n",
    "# We will use PyTorch and torchvision, which are a great combination for object detection.\n",
    "# MattNet is a research model, so we'll clone a public implementation from GitHub.\n",
    "\n",
    "!pip install torch torchvision\n",
    "!git clone https://github.com/lichengunc/MAttNet.git\n",
    "# Note: You may need to compile some custom C++/CUDA extensions depending on the MattNet implementation.\n",
    "# This can be done by navigating into the lib directory of the cloned repository and running:\n",
    "# cd MAttNet/lib\n",
    "# python setup.py build develop\n",
    "\n",
    "# Import all necessary libraries for the rest of the notebook.\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Define the device to use (GPU if available).\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b51ba760",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-20T19:54:42.475448Z",
     "iopub.status.busy": "2025-08-20T19:54:42.475121Z",
     "iopub.status.idle": "2025-08-20T19:59:26.685069Z",
     "shell.execute_reply": "2025-08-20T19:59:26.684348Z"
    },
    "papermill": {
     "duration": 284.248486,
     "end_time": "2025-08-20T19:59:26.704246",
     "exception": false,
     "start_time": "2025-08-20T19:54:42.455760",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Kaggle data root: /kaggle/input/visualgenome\n",
      "Searching for data files and image directories within the downloaded dataset...\n",
      "region_graphs.json found at: /kaggle/input/visualgenome/region_graphs.json/region_graphs.json\n",
      "image_data.json found at: /kaggle/input/visualgenome/image_data.json/image_data.json\n",
      "Image directory 1 found in: /kaggle/input/visualgenome/images/VG_100K\n",
      "Image directory 2 found in: /kaggle/input/visualgenome/images2/VG_100K_2\n",
      "Loaded 108077 region graph entries.\n",
      "Loaded 108077 image data entries.\n",
      "Using a subset of 50000 images for training.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell 2: Data Linking and Loading\n",
    "# This section has been updated to specifically handle the Kaggle file path structure you provided.\n",
    "# We will no longer use kagglehub but instead assume the dataset is mounted.\n",
    "\n",
    "# --- START OF FILE PATH FIX ---\n",
    "# Import necessary modules here to ensure they are always available.\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Define the root directory of the Kaggle dataset.\n",
    "KAGGLE_DATA_ROOT = \"/kaggle/input/visualgenome\"\n",
    "print(f\"Using Kaggle data root: {KAGGLE_DATA_ROOT}\")\n",
    "\n",
    "# We will now search for the specific file and image directories,\n",
    "# as they are located in different subdirectories.\n",
    "region_graphs_path = None\n",
    "image_data_path = None\n",
    "IMAGE_DIR_1 = None\n",
    "IMAGE_DIR_2 = None\n",
    "\n",
    "print(\"Searching for data files and image directories within the downloaded dataset...\")\n",
    "\n",
    "# Walk the directory tree to find all necessary files and directories.\n",
    "for root, dirs, files in os.walk(KAGGLE_DATA_ROOT):\n",
    "    # Find the path for region_graphs.json\n",
    "    if 'region_graphs.json' in files and region_graphs_path is None:\n",
    "        region_graphs_path = os.path.join(root, 'region_graphs.json')\n",
    "    \n",
    "    # Find the path for image_data.json\n",
    "    if 'image_data.json' in files and image_data_path is None:\n",
    "        image_data_path = os.path.join(root, 'image_data.json')\n",
    "\n",
    "    # Find the image directories.\n",
    "    if 'VG_100K' in dirs and IMAGE_DIR_1 is None:\n",
    "        IMAGE_DIR_1 = os.path.join(root, 'VG_100K')\n",
    "    if 'VG_100K_2' in dirs and IMAGE_DIR_2 is None:\n",
    "        IMAGE_DIR_2 = os.path.join(root, 'VG_100K_2')\n",
    "\n",
    "    # If all paths are found, we can stop the walk early to save time.\n",
    "    if region_graphs_path and image_data_path and IMAGE_DIR_1 and IMAGE_DIR_2:\n",
    "        break\n",
    "\n",
    "if not region_graphs_path:\n",
    "    raise FileNotFoundError(\"Could not locate the file 'region_graphs.json'.\")\n",
    "if not image_data_path:\n",
    "    raise FileNotFoundError(\"Could not locate the file 'image_data.json'.\")\n",
    "if not IMAGE_DIR_1:\n",
    "    raise FileNotFoundError(\"Could not locate the directory 'VG_100K'.\")\n",
    "if not IMAGE_DIR_2:\n",
    "    raise FileNotFoundError(\"Could not locate the directory 'VG_100K_2'.\")\n",
    "\n",
    "print(f\"region_graphs.json found at: {region_graphs_path}\")\n",
    "print(f\"image_data.json found at: {image_data_path}\")\n",
    "print(f\"Image directory 1 found in: {IMAGE_DIR_1}\")\n",
    "print(f\"Image directory 2 found in: {IMAGE_DIR_2}\")\n",
    "\n",
    "# Load the region graphs JSON data, which contains the annotations.\n",
    "with open(region_graphs_path, 'r') as f:\n",
    "    region_graphs = json.load(f)\n",
    "\n",
    "# Load the image data JSON data (if needed for metadata, e.g., image dimensions).\n",
    "with open(image_data_path, 'r') as f:\n",
    "    image_data = json.load(f)\n",
    "\n",
    "# --- END OF FILE PATH FIX ---\n",
    "\n",
    "print(f\"Loaded {len(region_graphs)} region graph entries.\")\n",
    "print(f\"Loaded {len(image_data)} image data entries.\")\n",
    "\n",
    "# Create a mapping from image ID to its file path for quick lookup.\n",
    "# We need to check both image directories.\n",
    "image_id_to_path = {}\n",
    "for img in image_data:\n",
    "    image_id = img['image_id']\n",
    "    path1 = os.path.join(IMAGE_DIR_1, f\"{image_id}.jpg\")\n",
    "    path2 = os.path.join(IMAGE_DIR_2, f\"{image_id}.jpg\")\n",
    "    if os.path.exists(path1):\n",
    "        image_id_to_path[image_id] = path1\n",
    "    elif os.path.exists(path2):\n",
    "        image_id_to_path[image_id] = path2\n",
    "\n",
    "# Prepare a list of all images we want to process.\n",
    "# Due to the large size, it's highly recommended to start with a subset.\n",
    "# For example, let's take the first 1000 images. For the final training, you would use all 50,000.\n",
    "# The code is designed to scale; just change the slice.\n",
    "subset_size = 50000\n",
    "image_ids = list(image_id_to_path.keys())[:subset_size]\n",
    "\n",
    "if not image_ids:\n",
    "    raise ValueError(\"No images were found in the dataset. Please check the dataset structure.\")\n",
    "print(f\"Using a subset of {len(image_ids)} images for training.\")\n",
    "\n",
    "\n",
    "# Create a dictionary to map image IDs to their descriptions for easier access.\n",
    "image_annotations = {}\n",
    "for entry in region_graphs:\n",
    "    image_id = entry['image_id']\n",
    "    if image_id in image_id_to_path:\n",
    "        # Each entry has a list of regions. We'll store them.\n",
    "        image_annotations[image_id] = entry['regions']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57c36a41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-20T19:59:26.741170Z",
     "iopub.status.busy": "2025-08-20T19:59:26.740543Z",
     "iopub.status.idle": "2025-08-20T19:59:26.783952Z",
     "shell.execute_reply": "2025-08-20T19:59:26.783009Z"
    },
    "papermill": {
     "duration": 0.062977,
     "end_time": "2025-08-20T19:59:26.785281",
     "exception": false,
     "start_time": "2025-08-20T19:59:26.722304",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader created with batch_size=2 and 4 workers.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Custom Dataset Class\n",
    "# This class handles loading each image and its corresponding annotations on-the-fly.\n",
    "# This is a critical step for memory efficiency with large datasets.\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os # Ensure os is imported for num_cpus\n",
    "\n",
    "class VisualGenomeDataset(Dataset):\n",
    "    def __init__(self, image_ids, image_annotations, image_id_to_path, transforms=None):\n",
    "        self.image_ids = image_ids\n",
    "        self.image_annotations = image_annotations\n",
    "        self.image_id_to_path = image_id_to_path\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Use a try-except block to gracefully handle potential issues with image loading or annotations\n",
    "        # which can sometimes be corrupt in large datasets.\n",
    "        try:\n",
    "            # Get the image ID for the current index.\n",
    "            image_id = self.image_ids[idx]\n",
    "            \n",
    "            # Open the image file.\n",
    "            img_path = self.image_id_to_path[image_id]\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            \n",
    "            # Get the annotations (regions) for this image.\n",
    "            regions = self.image_annotations.get(image_id, [])\n",
    "\n",
    "            # Parse the bounding boxes and labels from the regions.\n",
    "            boxes = []\n",
    "            labels = []\n",
    "            # The first class (0) is traditionally reserved for the background in PyTorch.\n",
    "            # We will use class 1 for all objects.\n",
    "            for region in regions:\n",
    "                x, y, w, h = region['x'], region['y'], region['width'], region['height']\n",
    "                # Convert (x, y, w, h) to (x_min, y_min, x_max, y_max)\n",
    "                boxes.append([x, y, x + w, y + h])\n",
    "                # For simplicity, we are assigning all as a single class (e.g., 'object').\n",
    "                labels.append(1) \n",
    "            \n",
    "            # If no objects are found, use a dummy box and label to prevent errors.\n",
    "            if not boxes:\n",
    "                boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "                labels = torch.zeros(0, dtype=torch.int64)\n",
    "            else:\n",
    "                boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "                labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "            target = {}\n",
    "            target[\"boxes\"] = boxes\n",
    "            target[\"labels\"] = labels\n",
    "            target[\"image_id\"] = torch.tensor([image_id])\n",
    "\n",
    "            # Apply transformations if they are defined.\n",
    "            if self.transforms is not None:\n",
    "                img = self.transforms(img)\n",
    "\n",
    "            return img, target\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image ID {self.image_ids[idx]}: {e}\")\n",
    "            # Return a valid empty sample to prevent the DataLoader from crashing.\n",
    "            return (torch.zeros((3, 224, 224), dtype=torch.float32), \n",
    "                    {'boxes': torch.zeros((0, 4), dtype=torch.float32), 'labels': torch.zeros(0, dtype=torch.int64)})\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "# Define transformations for the dataset.\n",
    "def get_transform():\n",
    "    return torchvision.transforms.ToTensor()\n",
    "\n",
    "# Create the dataset and a data loader. The data loader will handle batching and shuffling.\n",
    "if 'image_ids' in locals() and 'image_annotations' in locals() and 'image_id_to_path' in locals():\n",
    "    dataset = VisualGenomeDataset(image_ids, image_annotations, image_id_to_path, transforms=get_transform())\n",
    "\n",
    "    # A custom collate function is needed because the images have different numbers of objects.\n",
    "    def collate_fn(batch):\n",
    "        return tuple(zip(*batch))\n",
    "\n",
    "    # --- OPTIMIZED DATALOADER ---\n",
    "# Using multiple workers to load data in parallel and pinning memory for faster GPU transfer.\n",
    "num_cpus = os.cpu_count()\n",
    "data_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=2,  # CHANGED: Lowered from 4 to 2 to reduce memory usage.\n",
    "    shuffle=True,\n",
    "    num_workers=min(num_cpus, 8),\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "print(f\"DataLoader created with batch_size=2 and {min(num_cpus, 8)} workers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16ce4dda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-20T19:59:26.822850Z",
     "iopub.status.busy": "2025-08-20T19:59:26.822590Z",
     "iopub.status.idle": "2025-08-20T19:59:26.826712Z",
     "shell.execute_reply": "2025-08-20T19:59:26.826068Z"
    },
    "papermill": {
     "duration": 0.023903,
     "end_time": "2025-08-20T19:59:26.827781",
     "exception": false,
     "start_time": "2025-08-20T19:59:26.803878",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch compiler configured for better performance.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3.5: Compiler Configuration\n",
    "# We add this cell to optimize the behavior of torch.compile for this specific model.\n",
    "\n",
    "import torch\n",
    "\n",
    "# Increase the cache limit for compiled models to handle variable image sizes.\n",
    "torch._dynamo.config.cache_size_limit = 64\n",
    "\n",
    "# Instruct dynamo to capture scalar outputs like .item(), preventing graph breaks.\n",
    "torch._dynamo.config.capture_scalar_outputs = True\n",
    "\n",
    "print(\"PyTorch compiler configured for better performance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbabd3d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-20T19:59:26.864724Z",
     "iopub.status.busy": "2025-08-20T19:59:26.864475Z",
     "iopub.status.idle": "2025-08-20T19:59:28.605384Z",
     "shell.execute_reply": "2025-08-20T19:59:28.604551Z"
    },
    "papermill": {
     "duration": 1.760629,
     "end_time": "2025-08-20T19:59:28.606674",
     "exception": false,
     "start_time": "2025-08-20T19:59:26.846045",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
      "100%|██████████| 160M/160M [00:00<00:00, 230MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully. Proceeding without torch.compile.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Model Initialization (Final Version)\n",
    "\n",
    "# The number of classes: 1 for 'object' and 1 for the background.\n",
    "num_classes = 2\n",
    "\n",
    "# Load a pre-trained Faster R-CNN model from torchvision.\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights='DEFAULT')\n",
    "\n",
    "# Replace the box predictor to match our number of classes.\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# --- The torch.compile lines have been removed to prevent the BackendCompilerFailed error ---\n",
    "print(\"Model loaded successfully. Proceeding without torch.compile.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52490dc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-20T19:59:28.645021Z",
     "iopub.status.busy": "2025-08-20T19:59:28.644817Z",
     "iopub.status.idle": "2025-08-21T07:41:12.598345Z",
     "shell.execute_reply": "2025-08-21T07:41:12.597547Z"
    },
    "papermill": {
     "duration": 42103.974065,
     "end_time": "2025-08-21T07:41:12.599544",
     "exception": false,
     "start_time": "2025-08-20T19:59:28.625479",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting optimized training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   0%|          | 0/25000 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "Epoch 1/5: 100%|██████████| 25000/25000 [2:29:30<00:00,  2.79it/s, loss=1.56, lr=0.00076]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 25000/25000 [2:28:22<00:00,  2.81it/s, loss=1.39, lr=0.00095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 25000/25000 [2:15:53<00:00,  3.07it/s, loss=nan, lr=0.000611]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 25000/25000 [2:13:56<00:00,  3.11it/s, loss=nan, lr=0.000188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 25000/25000 [2:14:00<00:00,  3.11it/s, loss=nan, lr=4e-9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 finished.\n",
      "Training complete. Optimized model saved.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Training Pipeline (Final Optimized Version)\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "num_epochs = 5\n",
    "# Effective batch size = batch_size * accumulation_steps (2 * 16 = 32)\n",
    "accumulation_steps = 16 # CHANGED: Increased from 8 to 16 to maintain training stability.\n",
    "\n",
    "# --- Optimizer & Scheduler ---\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "total_steps = len(data_loader) * num_epochs \n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001, total_steps=total_steps)\n",
    "\n",
    "# --- Mixed Precision Scaler (Updated API) ---\n",
    "# Use torch.amp.GradScaler for the modern, non-deprecated API.\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "# --- Main Training Loop ---\n",
    "print(\"Starting optimized training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    progress_bar = tqdm(data_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for i, batch in enumerate(progress_bar):\n",
    "        if batch[0] is None:\n",
    "            continue\n",
    "        images, targets = batch\n",
    "        \n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Use autocast with the modern, non-deprecated API.\n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            \n",
    "            if accumulation_steps > 1:\n",
    "                losses = losses / accumulation_steps\n",
    "\n",
    "        scaler.scale(losses).backward()\n",
    "\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # NOTE: The warning about calling lr_scheduler.step() before optimizer.step()\n",
    "        # is expected and correct behavior when using OneCycleLR with gradient accumulation.\n",
    "        # We want the learning rate to update smoothly every batch.\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix(loss=(losses.item() * accumulation_steps), lr=optimizer.param_groups[0]['lr'])\n",
    "\n",
    "    print(f\"Epoch {epoch+1} finished.\")\n",
    "\n",
    "# --- Save Model ---\n",
    "torch.save(model.state_dict(), 'faster_rcnn_visual_genome_optimized.pth')\n",
    "print(\"Training complete. Optimized model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fbca4a",
   "metadata": {
    "papermill": {
     "duration": 10.954995,
     "end_time": "2025-08-21T07:41:33.936037",
     "exception": false,
     "start_time": "2025-08-21T07:41:22.981042",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1312522,
     "sourceId": 2186453,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 42510.816952,
   "end_time": "2025-08-21T07:41:48.784232",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-20T19:53:17.967280",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
