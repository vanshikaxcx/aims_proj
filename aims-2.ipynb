{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f55cb95b",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-16T21:06:54.732123Z",
     "iopub.status.busy": "2025-08-16T21:06:54.731845Z",
     "iopub.status.idle": "2025-08-16T21:08:32.743872Z",
     "shell.execute_reply": "2025-08-16T21:08:32.743046Z"
    },
    "papermill": {
     "duration": 98.041031,
     "end_time": "2025-08-16T21:08:32.769421",
     "exception": false,
     "start_time": "2025-08-16T21:06:54.728390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\r\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.5.1)\r\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\r\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\r\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\r\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\r\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\r\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\r\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\r\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\r\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\r\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\r\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\r\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\r\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\r\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\r\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-curand-cu12\r\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\r\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\r\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\r\n",
      "  Attempting uninstall: nvidia-cufft-cu12\r\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\r\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\r\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\r\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\r\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\r\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\r\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cublas-cu12\r\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\r\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\r\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\r\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\r\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\r\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\r\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\r\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\r\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\r\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\r\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\r\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\r\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\r\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\r\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\r\n",
      "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\r\n",
      "Cloning into 'MAttNet'...\r\n",
      "remote: Enumerating objects: 337, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (12/12), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (12/12), done.\u001b[K\r\n",
      "remote: Total 337 (delta 6), reused 1 (delta 0), pack-reused 325 (from 1)\u001b[K\r\n",
      "Receiving objects: 100% (337/337), 2.24 MiB | 16.15 MiB/s, done.\r\n",
      "Resolving deltas: 100% (184/184), done.\r\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup and Installations\n",
    "# This first cell sets up the environment by installing necessary libraries and cloning the required repositories.\n",
    "# We will use PyTorch and torchvision, which are a great combination for object detection.\n",
    "# MattNet is a research model, so we'll clone a public implementation from GitHub.\n",
    "\n",
    "!pip install torch torchvision\n",
    "!git clone https://github.com/lichengunc/MAttNet.git\n",
    "# Note: You may need to compile some custom C++/CUDA extensions depending on the MattNet implementation.\n",
    "# This can be done by navigating into the lib directory of the cloned repository and running:\n",
    "# cd MAttNet/lib\n",
    "# python setup.py build develop\n",
    "\n",
    "# Import all necessary libraries for the rest of the notebook.\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Define the device to use (GPU if available).\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0021b886",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T21:08:32.820000Z",
     "iopub.status.busy": "2025-08-16T21:08:32.819462Z",
     "iopub.status.idle": "2025-08-16T21:15:07.518042Z",
     "shell.execute_reply": "2025-08-16T21:15:07.517181Z"
    },
    "papermill": {
     "duration": 394.751747,
     "end_time": "2025-08-16T21:15:07.545817",
     "exception": false,
     "start_time": "2025-08-16T21:08:32.794070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Kaggle data root: /kaggle/input/visualgenome\n",
      "Searching for data files and image directories within the downloaded dataset...\n",
      "region_graphs.json found at: /kaggle/input/visualgenome/region_graphs.json/region_graphs.json\n",
      "image_data.json found at: /kaggle/input/visualgenome/image_data.json/image_data.json\n",
      "Image directory 1 found in: /kaggle/input/visualgenome/images/VG_100K\n",
      "Image directory 2 found in: /kaggle/input/visualgenome/images2/VG_100K_2\n",
      "Loaded 108077 region graph entries.\n",
      "Loaded 108077 image data entries.\n",
      "Using a subset of 50000 images for training.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell 2: Data Linking and Loading\n",
    "# This section has been updated to specifically handle the Kaggle file path structure you provided.\n",
    "# We will no longer use kagglehub but instead assume the dataset is mounted.\n",
    "\n",
    "# --- START OF FILE PATH FIX ---\n",
    "# Import necessary modules here to ensure they are always available.\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Define the root directory of the Kaggle dataset.\n",
    "KAGGLE_DATA_ROOT = \"/kaggle/input/visualgenome\"\n",
    "print(f\"Using Kaggle data root: {KAGGLE_DATA_ROOT}\")\n",
    "\n",
    "# We will now search for the specific file and image directories,\n",
    "# as they are located in different subdirectories.\n",
    "region_graphs_path = None\n",
    "image_data_path = None\n",
    "IMAGE_DIR_1 = None\n",
    "IMAGE_DIR_2 = None\n",
    "\n",
    "print(\"Searching for data files and image directories within the downloaded dataset...\")\n",
    "\n",
    "# Walk the directory tree to find all necessary files and directories.\n",
    "for root, dirs, files in os.walk(KAGGLE_DATA_ROOT):\n",
    "    # Find the path for region_graphs.json\n",
    "    if 'region_graphs.json' in files and region_graphs_path is None:\n",
    "        region_graphs_path = os.path.join(root, 'region_graphs.json')\n",
    "    \n",
    "    # Find the path for image_data.json\n",
    "    if 'image_data.json' in files and image_data_path is None:\n",
    "        image_data_path = os.path.join(root, 'image_data.json')\n",
    "\n",
    "    # Find the image directories.\n",
    "    if 'VG_100K' in dirs and IMAGE_DIR_1 is None:\n",
    "        IMAGE_DIR_1 = os.path.join(root, 'VG_100K')\n",
    "    if 'VG_100K_2' in dirs and IMAGE_DIR_2 is None:\n",
    "        IMAGE_DIR_2 = os.path.join(root, 'VG_100K_2')\n",
    "\n",
    "    # If all paths are found, we can stop the walk early to save time.\n",
    "    if region_graphs_path and image_data_path and IMAGE_DIR_1 and IMAGE_DIR_2:\n",
    "        break\n",
    "\n",
    "if not region_graphs_path:\n",
    "    raise FileNotFoundError(\"Could not locate the file 'region_graphs.json'.\")\n",
    "if not image_data_path:\n",
    "    raise FileNotFoundError(\"Could not locate the file 'image_data.json'.\")\n",
    "if not IMAGE_DIR_1:\n",
    "    raise FileNotFoundError(\"Could not locate the directory 'VG_100K'.\")\n",
    "if not IMAGE_DIR_2:\n",
    "    raise FileNotFoundError(\"Could not locate the directory 'VG_100K_2'.\")\n",
    "\n",
    "print(f\"region_graphs.json found at: {region_graphs_path}\")\n",
    "print(f\"image_data.json found at: {image_data_path}\")\n",
    "print(f\"Image directory 1 found in: {IMAGE_DIR_1}\")\n",
    "print(f\"Image directory 2 found in: {IMAGE_DIR_2}\")\n",
    "\n",
    "# Load the region graphs JSON data, which contains the annotations.\n",
    "with open(region_graphs_path, 'r') as f:\n",
    "    region_graphs = json.load(f)\n",
    "\n",
    "# Load the image data JSON data (if needed for metadata, e.g., image dimensions).\n",
    "with open(image_data_path, 'r') as f:\n",
    "    image_data = json.load(f)\n",
    "\n",
    "# --- END OF FILE PATH FIX ---\n",
    "\n",
    "print(f\"Loaded {len(region_graphs)} region graph entries.\")\n",
    "print(f\"Loaded {len(image_data)} image data entries.\")\n",
    "\n",
    "# Create a mapping from image ID to its file path for quick lookup.\n",
    "# We need to check both image directories.\n",
    "image_id_to_path = {}\n",
    "for img in image_data:\n",
    "    image_id = img['image_id']\n",
    "    path1 = os.path.join(IMAGE_DIR_1, f\"{image_id}.jpg\")\n",
    "    path2 = os.path.join(IMAGE_DIR_2, f\"{image_id}.jpg\")\n",
    "    if os.path.exists(path1):\n",
    "        image_id_to_path[image_id] = path1\n",
    "    elif os.path.exists(path2):\n",
    "        image_id_to_path[image_id] = path2\n",
    "\n",
    "# Prepare a list of all images we want to process.\n",
    "# Due to the large size, it's highly recommended to start with a subset.\n",
    "# For example, let's take the first 1000 images. For the final training, you would use all 50,000.\n",
    "# The code is designed to scale; just change the slice.\n",
    "subset_size = 50000\n",
    "image_ids = list(image_id_to_path.keys())[:subset_size]\n",
    "\n",
    "if not image_ids:\n",
    "    raise ValueError(\"No images were found in the dataset. Please check the dataset structure.\")\n",
    "print(f\"Using a subset of {len(image_ids)} images for training.\")\n",
    "\n",
    "\n",
    "# Create a dictionary to map image IDs to their descriptions for easier access.\n",
    "image_annotations = {}\n",
    "for entry in region_graphs:\n",
    "    image_id = entry['image_id']\n",
    "    if image_id in image_id_to_path:\n",
    "        # Each entry has a list of regions. We'll store them.\n",
    "        image_annotations[image_id] = entry['regions']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9493127",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T21:15:07.596716Z",
     "iopub.status.busy": "2025-08-16T21:15:07.596272Z",
     "iopub.status.idle": "2025-08-16T21:15:07.624415Z",
     "shell.execute_reply": "2025-08-16T21:15:07.623820Z"
    },
    "papermill": {
     "duration": 0.054957,
     "end_time": "2025-08-16T21:15:07.625452",
     "exception": false,
     "start_time": "2025-08-16T21:15:07.570495",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader created with 4 workers.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Custom Dataset Class\n",
    "# This class handles loading each image and its corresponding annotations on-the-fly.\n",
    "# This is a critical step for memory efficiency with large datasets.\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os # Ensure os is imported for num_cpus\n",
    "\n",
    "class VisualGenomeDataset(Dataset):\n",
    "    def __init__(self, image_ids, image_annotations, image_id_to_path, transforms=None):\n",
    "        self.image_ids = image_ids\n",
    "        self.image_annotations = image_annotations\n",
    "        self.image_id_to_path = image_id_to_path\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Use a try-except block to gracefully handle potential issues with image loading or annotations\n",
    "        # which can sometimes be corrupt in large datasets.\n",
    "        try:\n",
    "            # Get the image ID for the current index.\n",
    "            image_id = self.image_ids[idx]\n",
    "            \n",
    "            # Open the image file.\n",
    "            img_path = self.image_id_to_path[image_id]\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            \n",
    "            # Get the annotations (regions) for this image.\n",
    "            regions = self.image_annotations.get(image_id, [])\n",
    "\n",
    "            # Parse the bounding boxes and labels from the regions.\n",
    "            boxes = []\n",
    "            labels = []\n",
    "            # The first class (0) is traditionally reserved for the background in PyTorch.\n",
    "            # We will use class 1 for all objects.\n",
    "            for region in regions:\n",
    "                x, y, w, h = region['x'], region['y'], region['width'], region['height']\n",
    "                # Convert (x, y, w, h) to (x_min, y_min, x_max, y_max)\n",
    "                boxes.append([x, y, x + w, y + h])\n",
    "                # For simplicity, we are assigning all as a single class (e.g., 'object').\n",
    "                labels.append(1) \n",
    "            \n",
    "            # If no objects are found, use a dummy box and label to prevent errors.\n",
    "            if not boxes:\n",
    "                boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "                labels = torch.zeros(0, dtype=torch.int64)\n",
    "            else:\n",
    "                boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "                labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "            target = {}\n",
    "            target[\"boxes\"] = boxes\n",
    "            target[\"labels\"] = labels\n",
    "            target[\"image_id\"] = torch.tensor([image_id])\n",
    "\n",
    "            # Apply transformations if they are defined.\n",
    "            if self.transforms is not None:\n",
    "                img = self.transforms(img)\n",
    "\n",
    "            return img, target\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image ID {self.image_ids[idx]}: {e}\")\n",
    "            # Return a valid empty sample to prevent the DataLoader from crashing.\n",
    "            return (torch.zeros((3, 224, 224), dtype=torch.float32), \n",
    "                    {'boxes': torch.zeros((0, 4), dtype=torch.float32), 'labels': torch.zeros(0, dtype=torch.int64)})\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "# Define transformations for the dataset.\n",
    "def get_transform():\n",
    "    return torchvision.transforms.ToTensor()\n",
    "\n",
    "# Create the dataset and a data loader. The data loader will handle batching and shuffling.\n",
    "if 'image_ids' in locals() and 'image_annotations' in locals() and 'image_id_to_path' in locals():\n",
    "    dataset = VisualGenomeDataset(image_ids, image_annotations, image_id_to_path, transforms=get_transform())\n",
    "\n",
    "    # A custom collate function is needed because the images have different numbers of objects.\n",
    "    def collate_fn(batch):\n",
    "        return tuple(zip(*batch))\n",
    "\n",
    "    # --- OPTIMIZED DATALOADER ---\n",
    "    # Using multiple workers to load data in parallel and pinning memory for faster GPU transfer.\n",
    "    num_cpus = os.cpu_count()\n",
    "    data_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=4,  # Increased batch size slightly, we'll manage memory with accumulation.\n",
    "        shuffle=True,\n",
    "        num_workers=min(num_cpus, 8), # Use multiple CPU cores to load data. THIS IS A KEY SPEEDUP.\n",
    "        pin_memory=True,             # Speeds up CPU-to-GPU memory transfer.\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    print(f\"DataLoader created with {min(num_cpus, 8)} workers.\")\n",
    "else:\n",
    "    print(\"Skipping Dataset and DataLoader creation due to missing data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94998f8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T21:15:07.721195Z",
     "iopub.status.busy": "2025-08-16T21:15:07.720924Z",
     "iopub.status.idle": "2025-08-16T21:15:11.477137Z",
     "shell.execute_reply": "2025-08-16T21:15:11.476245Z"
    },
    "papermill": {
     "duration": 3.782332,
     "end_time": "2025-08-16T21:15:11.478388",
     "exception": false,
     "start_time": "2025-08-16T21:15:07.696056",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
      "100%|██████████| 160M/160M [00:00<00:00, 189MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling the model... (this may take a minute)\n",
      "Model compiled successfully.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Model Initialization (Faster R-CNN with MattNet Concept)\n",
    "\n",
    "# The number of classes: 1 for 'object' and 1 for the background.\n",
    "num_classes = 2\n",
    "\n",
    "# Load a pre-trained Faster R-CNN model from torchvision.\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights='DEFAULT') # Use modern weights argument\n",
    "\n",
    "# Replace the box predictor to match our number of classes.\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# --- OPTIMIZATION ---\n",
    "# Add torch.compile() for a JIT compilation speedup (requires PyTorch 2.0+).\n",
    "print(\"Compiling the model... (this may take a minute)\")\n",
    "model = torch.compile(model)\n",
    "print(\"Model compiled successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d3b907f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T21:15:11.531800Z",
     "iopub.status.busy": "2025-08-16T21:15:11.531053Z",
     "iopub.status.idle": "2025-08-17T06:01:48.330311Z",
     "shell.execute_reply": "2025-08-17T06:01:48.329321Z"
    },
    "papermill": {
     "duration": 31599.936419,
     "end_time": "2025-08-17T06:01:51.440846",
     "exception": false,
     "start_time": "2025-08-16T21:15:11.504427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/3959337196.py:21: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting optimized training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   0%|          | 0/12500 [00:00<?, ?it/s]/tmp/ipykernel_19/3959337196.py:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "W0816 21:15:12.676000 19 torch/_dynamo/variables/tensor.py:869] [0/0] Graph break from `Tensor.item()`, consider setting:\n",
      "W0816 21:15:12.676000 19 torch/_dynamo/variables/tensor.py:869] [0/0]     torch._dynamo.config.capture_scalar_outputs = True\n",
      "W0816 21:15:12.676000 19 torch/_dynamo/variables/tensor.py:869] [0/0] or:\n",
      "W0816 21:15:12.676000 19 torch/_dynamo/variables/tensor.py:869] [0/0]     env TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1\n",
      "W0816 21:15:12.676000 19 torch/_dynamo/variables/tensor.py:869] [0/0] to include these operations in the captured graph.\n",
      "W0816 21:15:12.676000 19 torch/_dynamo/variables/tensor.py:869] [0/0] \n",
      "W0816 21:15:12.676000 19 torch/_dynamo/variables/tensor.py:869] [0/0] Graph break: from user code at:\n",
      "W0816 21:15:12.676000 19 torch/_dynamo/variables/tensor.py:869] [0/0]   File \"/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/generalized_rcnn.py\", line 83, in forward\n",
      "W0816 21:15:12.676000 19 torch/_dynamo/variables/tensor.py:869] [0/0]     images, targets = self.transform(images, targets)\n",
      "W0816 21:15:12.676000 19 torch/_dynamo/variables/tensor.py:869] [0/0]   File \"/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/transform.py\", line 142, in forward\n",
      "W0816 21:15:12.676000 19 torch/_dynamo/variables/tensor.py:869] [0/0]     image, target_index = self.resize(image, target_index)\n",
      "W0816 21:15:12.676000 19 torch/_dynamo/variables/tensor.py:869] [0/0]   File \"/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/transform.py\", line 188, in resize\n",
      "W0816 21:15:12.676000 19 torch/_dynamo/variables/tensor.py:869] [0/0]     size = self.torch_choice(self.min_size)\n",
      "W0816 21:15:12.676000 19 torch/_dynamo/variables/tensor.py:869] [0/0]   File \"/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/transform.py\", line 176, in torch_choice\n",
      "W0816 21:15:12.676000 19 torch/_dynamo/variables/tensor.py:869] [0/0]     index = int(torch.empty(1).uniform_(0.0, float(len(k))).item())\n",
      "W0816 21:15:12.676000 19 torch/_dynamo/variables/tensor.py:869] [0/0] \n",
      "W0816 21:15:12.676000 19 torch/_dynamo/variables/tensor.py:869] [0/0] \n",
      "W0816 21:16:09.533000 19 torch/_inductor/utils.py:1137] [10/0] Not enough SMs to use max_autotune_gemm mode\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/_meta_registrations.py:173: FutureWarning: `create_unbacked_symint` is deprecated, please use `new_dynamic_size` instead\n",
      "  num_to_keep = ctx.create_unbacked_symint()\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/_meta_registrations.py:173: FutureWarning: `create_unbacked_symint` is deprecated, please use `new_dynamic_size` instead\n",
      "  num_to_keep = ctx.create_unbacked_symint()\n",
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "Epoch 1/5:   0%|          | 1/12500 [03:19<691:16:22, 199.10s/it, loss=4.84, lr=4e-5]/tmp/ipykernel_19/3959337196.py:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 1/5:   0%|          | 2/12500 [04:08<386:16:20, 111.26s/it, loss=4.3, lr=4e-5]/tmp/ipykernel_19/3959337196.py:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 1/5:   0%|          | 3/12500 [05:23<328:37:52, 94.67s/it, loss=4.32, lr=4e-5] /tmp/ipykernel_19/3959337196.py:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 1/5:   0%|          | 4/12500 [05:26<202:54:34, 58.46s/it, loss=5.03, lr=4e-5]/tmp/ipykernel_19/3959337196.py:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 1/5:   0%|          | 5/12500 [06:29<207:43:44, 59.85s/it, loss=4.36, lr=4e-5]/tmp/ipykernel_19/3959337196.py:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 1/5:   0%|          | 6/12500 [06:33<141:53:45, 40.89s/it, loss=4.18, lr=4e-5]/tmp/ipykernel_19/3959337196.py:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 1/5:   0%|          | 7/12500 [06:35<97:45:36, 28.17s/it, loss=5.58, lr=4e-5] /tmp/ipykernel_19/3959337196.py:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 1/5:   0%|          | 8/12500 [06:37<68:56:33, 19.87s/it, loss=4.35, lr=4e-5]/tmp/ipykernel_19/3959337196.py:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 1/5:   0%|          | 9/12500 [06:40<50:44:35, 14.62s/it, loss=3.06, lr=4e-5]/tmp/ipykernel_19/3959337196.py:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "W0816 21:21:51.871000 19 torch/_dynamo/convert_frame.py:906] [7/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "W0816 21:21:51.871000 19 torch/_dynamo/convert_frame.py:906] [7/8]    function: 'batch_images' (/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/transform.py:237)\n",
      "W0816 21:21:51.871000 19 torch/_dynamo/convert_frame.py:906] [7/8]    last reason: 7/0: tensor 'L['images'][0]' size mismatch at index 1. expected 1066, actual 800\n",
      "W0816 21:21:51.871000 19 torch/_dynamo/convert_frame.py:906] [7/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W0816 21:21:51.871000 19 torch/_dynamo/convert_frame.py:906] [7/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "Epoch 1/5:   0%|          | 10/12500 [06:40<35:33:09, 10.25s/it, loss=2.66, lr=4e-5]/tmp/ipykernel_19/3959337196.py:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 1/5:   0%|          | 13/12500 [06:58<29:53:58,  8.62s/it, loss=2.51, lr=4e-5]/tmp/ipykernel_19/3959337196.py:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 1/5:   1%|          | 72/12500 [08:33<64:05:26, 18.57s/it, loss=2.14, lr=4e-5]/tmp/ipykernel_19/3959337196.py:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 1/5:   1%|          | 80/12500 [08:39<7:36:36,  2.21s/it, loss=1.71, lr=4e-5]/tmp/ipykernel_19/3959337196.py:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 1/5:   1%|          | 97/12500 [09:03<17:09:59,  4.98s/it, loss=1.78, lr=4.01e-5]/tmp/ipykernel_19/3959337196.py:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 1/5:   2%|▏         | 250/12500 [10:46<17:51:31,  5.25s/it, loss=1.63, lr=4.04e-5]/tmp/ipykernel_19/3959337196.py:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 1/5:  47%|████▋     | 5935/12500 [1:03:43<57:57,  1.89it/s, loss=1.34, lr=0.000258]W0816 22:18:55.416000 19 torch/_dynamo/convert_frame.py:906] [11/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "W0816 22:18:55.416000 19 torch/_dynamo/convert_frame.py:906] [11/8]    function: 'forward' (/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/rpn.py:336)\n",
      "W0816 22:18:55.416000 19 torch/_dynamo/convert_frame.py:906] [11/8]    last reason: 11/0: tensor 'L['self']._modules['anchor_generator'].cell_anchors[0]' dispatch key set mismatch. expected DispatchKeySet(CPU, BackendSelect, ADInplaceOrView, AutogradCPU), actual DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA)\n",
      "W0816 22:18:55.416000 19 torch/_dynamo/convert_frame.py:906] [11/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W0816 22:18:55.416000 19 torch/_dynamo/convert_frame.py:906] [11/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "Epoch 1/5:  47%|████▋     | 5936/12500 [1:03:45<1:22:56,  1.32it/s, loss=1.52, lr=0.000258]/tmp/ipykernel_19/3959337196.py:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 1/5: 100%|██████████| 12500/12500 [2:04:30<00:00,  1.67it/s, loss=1.51, lr=0.00076]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:   5%|▌         | 659/12500 [06:11<9:28:05,  2.88s/it, loss=1.63, lr=0.000804]/tmp/ipykernel_19/3959337196.py:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 2/5:  49%|████▉     | 6129/12500 [56:23<57:50,  1.84it/s, loss=12, lr=0.001]  /usr/local/lib/python3.11/dist-packages/torchvision/_meta_registrations.py:173: FutureWarning: `create_unbacked_symint` is deprecated, please use `new_dynamic_size` instead\n",
      "  num_to_keep = ctx.create_unbacked_symint()\n",
      "Epoch 2/5:  49%|████▉     | 6130/12500 [56:24<1:06:20,  1.60it/s, loss=11.5, lr=0.001]/tmp/ipykernel_19/3959337196.py:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 2/5:  49%|████▉     | 6136/12500 [56:28<1:03:44,  1.66it/s, loss=12.6, lr=0.001]/usr/local/lib/python3.11/dist-packages/torchvision/_meta_registrations.py:173: FutureWarning: `create_unbacked_symint` is deprecated, please use `new_dynamic_size` instead\n",
      "  num_to_keep = ctx.create_unbacked_symint()\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/_meta_registrations.py:173: FutureWarning: `create_unbacked_symint` is deprecated, please use `new_dynamic_size` instead\n",
      "  num_to_keep = ctx.create_unbacked_symint()\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/_meta_registrations.py:173: FutureWarning: `create_unbacked_symint` is deprecated, please use `new_dynamic_size` instead\n",
      "  num_to_keep = ctx.create_unbacked_symint()\n",
      "Epoch 2/5:  49%|████▉     | 6137/12500 [56:50<12:50:50,  7.27s/it, loss=1.07e+3, lr=0.001]/tmp/ipykernel_19/3959337196.py:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 2/5:  49%|████▉     | 6164/12500 [57:10<4:31:03,  2.57s/it, loss=828, lr=0.001]/tmp/ipykernel_19/3959337196.py:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 2/5:  50%|████▉     | 6241/12500 [57:58<4:59:33,  2.87s/it, loss=nan, lr=0.001]/tmp/ipykernel_19/3959337196.py:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 2/5: 100%|██████████| 12500/12500 [1:47:12<00:00,  1.94it/s, loss=nan, lr=0.00095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  10%|█         | 1277/12500 [10:03<2:27:33,  1.27it/s, loss=nan, lr=0.000929]/tmp/ipykernel_19/3959337196.py:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 3/5: 100%|██████████| 12500/12500 [1:38:15<00:00,  2.12it/s, loss=nan, lr=0.000611]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 12500/12500 [1:38:28<00:00,  2.12it/s, loss=nan, lr=0.000188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 12500/12500 [1:38:10<00:00,  2.12it/s, loss=nan, lr=4e-9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 finished.\n",
      "Training complete. Optimized model saved.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Training Pipeline (Optimized)\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "num_epochs = 5\n",
    "# Gradient accumulation simulates a larger batch size.\n",
    "# Effective batch size = batch_size * accumulation_steps (e.g., 4 * 8 = 32)\n",
    "accumulation_steps = 8 \n",
    "\n",
    "# --- Optimizer & Scheduler ---\n",
    "# Use a modern optimizer and a more effective learning rate scheduler.\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "# OneCycleLR is highly effective and requires the total number of training steps.\n",
    "total_steps = len(data_loader) * num_epochs \n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001, total_steps=total_steps)\n",
    "\n",
    "# --- Mixed Precision Scaler ---\n",
    "# The GradScaler helps prevent numerical underflow with float16 gradients.\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# --- Main Training Loop ---\n",
    "print(\"Starting optimized training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    progress_bar = tqdm(data_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for i, batch in enumerate(progress_bar):\n",
    "        # Gracefully handle potential empty batches from the collate_fn\n",
    "        if batch[0] is None:\n",
    "            continue\n",
    "        images, targets = batch\n",
    "        \n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Use autocast for the forward pass (enables mixed precision)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            \n",
    "            # Scale the loss for gradient accumulation\n",
    "            if accumulation_steps > 1:\n",
    "                losses = losses / accumulation_steps\n",
    "\n",
    "        # Backward pass using the scaler\n",
    "        scaler.scale(losses).backward()\n",
    "\n",
    "        # Optimizer step only after accumulating gradients for `accumulation_steps` batches\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # Update learning rate scheduler every step\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        # Update progress bar with the unscaled loss and current learning rate\n",
    "        progress_bar.set_postfix(loss=(losses.item() * accumulation_steps), lr=optimizer.param_groups[0]['lr'])\n",
    "\n",
    "    print(f\"Epoch {epoch+1} finished.\")\n",
    "\n",
    "# --- Save Model ---\n",
    "# It's good practice to save the original model's state_dict before it was compiled\n",
    "torch.save(model.state_dict(), 'faster_rcnn_visual_genome_optimized.pth')\n",
    "print(\"Training complete. Optimized model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9ab2ea",
   "metadata": {
    "papermill": {
     "duration": 5.526068,
     "end_time": "2025-08-17T06:02:02.312470",
     "exception": false,
     "start_time": "2025-08-17T06:01:56.786402",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1312522,
     "sourceId": 2186453,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 32123.330192,
   "end_time": "2025-08-17T06:02:12.103867",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-16T21:06:48.773675",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
